{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d55adb3",
   "metadata": {},
   "source": [
    "# **0. Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ef3e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc15b5",
   "metadata": {},
   "source": [
    "# **1. Load Data and Basic Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c3f47",
   "metadata": {},
   "source": [
    "## **1.1 Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20065f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Audiobooks_data.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d229675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  Book_length_mins_overall  Book_length_mins_avg  Price_overall  Price_avg  Review  Review 10/10  \\\n",
      "0  873                    2160.0                  2160          10.13      10.13       0          8.91   \n",
      "1  611                    1404.0                  2808           6.66      13.33       1          6.50   \n",
      "2  705                     324.0                   324          10.13      10.13       1          9.00   \n",
      "3  391                    1620.0                  1620          15.31      15.31       0          9.00   \n",
      "4  819                     432.0                  1296           7.11      21.33       1          9.00   \n",
      "\n",
      "   Minutes_listened  Completion  Support_requests  Last_visited_Minus_Purchase_date  Targets  \n",
      "0               0.0         0.0                 0                                 0        1  \n",
      "1               0.0         0.0                 0                               182        1  \n",
      "2               0.0         0.0                 1                               334        1  \n",
      "3               0.0         0.0                 0                               183        1  \n",
      "4               0.0         0.0                 0                                 0        1  \n",
      "['ID', 'Book_length_mins_overall', 'Book_length_mins_avg', 'Price_overall', 'Price_avg', 'Review', 'Review 10/10', 'Minutes_listened', 'Completion', 'Support_requests', 'Last_visited_Minus_Purchase_date', 'Targets']\n"
     ]
    }
   ],
   "source": [
    "df.columns = [\n",
    "    \"ID\",\n",
    "    \"Book_length_mins_overall\",\n",
    "    \"Book_length_mins_avg\",\n",
    "    \"Price_overall\",\n",
    "    \"Price_avg\",\n",
    "    \"Review\",\n",
    "    \"Review 10/10\",\n",
    "    \"Minutes_listened\",\n",
    "    \"Completion\",\n",
    "    \"Support_requests\",\n",
    "    \"Last_visited_Minus_Purchase_date\",\n",
    "    \"Targets\",\n",
    "]\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fe73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (14084, 12)\n",
      "\n",
      "Targets value counts:\n",
      "Targets\n",
      "0    11847\n",
      "1     2237\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Nulos por columna (features):\n",
      "Book_length_mins_overall            0\n",
      "Book_length_mins_avg                0\n",
      "Price_overall                       0\n",
      "Price_avg                           0\n",
      "Review                              0\n",
      "Review 10/10                        0\n",
      "Minutes_listened                    0\n",
      "Completion                          0\n",
      "Support_requests                    0\n",
      "Last_visited_Minus_Purchase_date    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"Targets\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"Book_length_mins_overall\",\n",
    "    \"Book_length_mins_avg\",\n",
    "    \"Price_overall\",\n",
    "    \"Price_avg\",\n",
    "    \"Review\",\n",
    "    \"Review 10/10\",\n",
    "    \"Minutes_listened\",\n",
    "    \"Completion\",\n",
    "    \"Support_requests\",\n",
    "    \"Last_visited_Minus_Purchase_date\",\n",
    "]\n",
    "\n",
    "REQUIRED_COLS = [TARGET_COL] + FEATURES\n",
    "\n",
    "missing_cols = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Faltan columnas requeridas en df: {missing_cols}\")\n",
    "\n",
    "# Chequeos rápidos\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nTargets value counts:\")\n",
    "print(df[TARGET_COL].value_counts(dropna=False))\n",
    "\n",
    "# Revisa nulos en features\n",
    "print(\"\\nNulos por columna (features):\")\n",
    "print(df[FEATURES].isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# Asegura target binario (0/1)\n",
    "# Si Targets viene como True/False o 'yes'/'no', conviértelo explícitamente.\n",
    "unique_targets = df[TARGET_COL].dropna().unique()\n",
    "if len(unique_targets) > 2:\n",
    "    raise ValueError(f\"Targets no parece binario. Valores únicos: {unique_targets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed48184",
   "metadata": {},
   "source": [
    "# **2. Identify Temporal Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5615eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidatas a columna de tiempo: []\n"
     ]
    }
   ],
   "source": [
    "def find_datetime_candidates(dataframe: pd.DataFrame):\n",
    "    candidates = []\n",
    "    # 1) columnas ya en datetime\n",
    "    for col in dataframe.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(dataframe[col]):\n",
    "            candidates.append(col)\n",
    "    # 2) columnas tipo object con nombres sugerentes\n",
    "    name_hints = (\"date\", \"time\", \"timestamp\", \"ts\", \"created\", \"purchase\", \"visit\", \"event\")\n",
    "    for col in dataframe.select_dtypes(include=[\"object\"]).columns:\n",
    "        if any(h in col.lower() for h in name_hints):\n",
    "            candidates.append(col)\n",
    "    return sorted(set(candidates))\n",
    "\n",
    "datetime_candidates = find_datetime_candidates(df)\n",
    "print(\"\\nCandidatas a columna de tiempo:\", datetime_candidates)\n",
    "\n",
    "# Si eliges una (por ejemplo, \"Purchase_date\"), descomenta y ajusta:\n",
    "TIME_COL = None  # e.g., \"Purchase_date\"\n",
    "\n",
    "if TIME_COL is not None:\n",
    "    # Intentar convertir a datetime si no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[TIME_COL]):\n",
    "        df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
    "\n",
    "    # Si hay muchos NaT, cuidado\n",
    "    nat_rate = df[TIME_COL].isna().mean()\n",
    "    print(f\"\\nNaT rate en {TIME_COL}: {nat_rate:.3f}\")\n",
    "    if nat_rate > 0.2:\n",
    "        print(\"ADVERTENCIA: muchos valores no convertibles a fecha; split temporal puede ser poco fiable.\")\n",
    "\n",
    "    df = df.sort_values(TIME_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf8d89",
   "metadata": {},
   "source": [
    "# **3. Features Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb60a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[FEATURES].copy()\n",
    "y = df[TARGET_COL].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5852dd9",
   "metadata": {},
   "source": [
    "# **4. Split if Temporal Column Exists if not Stratified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474fe488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split estratificado aplicado.\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "if TIME_COL is not None:\n",
    "    # Holdout temporal: último 20% como test\n",
    "    n = len(df)\n",
    "    split_idx = int((1 - TEST_SIZE) * n)\n",
    "\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    print(\"\\nSplit temporal aplicado.\")\n",
    "    print(\"Train period:\", df[TIME_COL].iloc[:split_idx].min(), \"->\", df[TIME_COL].iloc[:split_idx].max())\n",
    "    print(\"Test  period:\", df[TIME_COL].iloc[split_idx:].min(), \"->\", df[TIME_COL].iloc[split_idx:].max())\n",
    "else:\n",
    "    # Split estratificado (mantiene proporción de clases)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(\"\\nSplit estratificado aplicado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c624087",
   "metadata": {},
   "source": [
    "# **5. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6a03cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numéricas: ['Book_length_mins_overall', 'Book_length_mins_avg', 'Price_overall', 'Price_avg', 'Review', 'Review 10/10', 'Minutes_listened', 'Completion', 'Support_requests', 'Last_visited_Minus_Purchase_date']\n",
      "Categóricas: []\n"
     ]
    }
   ],
   "source": [
    "numeric_features = [c for c in FEATURES if pd.api.types.is_numeric_dtype(df[c])]\n",
    "categorical_features = [c for c in FEATURES if c not in numeric_features]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    # Sin OneHotEncoder para mantenerlo simple si no aplica.\n",
    "    # Si detectas categóricas de verdad, activa OneHotEncoder:\n",
    "    # (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "print(\"\\nNuméricas:\", numeric_features)\n",
    "print(\"Categóricas:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9faaa3",
   "metadata": {},
   "source": [
    "# **6. Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2962e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", DummyClassifier(strategy=\"prior\", random_state=RANDOM_STATE))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b698756",
   "metadata": {},
   "source": [
    "# **7. Initial Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa25a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Regresión logística (interpretable, buen baseline fuerte)\n",
    "logreg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        solver=\"lbfgs\",\n",
    "        class_weight=None,   # si hay desbalance fuerte, puedes probar \"balanced\"\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "# (B) Modelo más potente: Gradient Boosting (tabular, no requiere escalado, pero aquí ya está en pipeline)\n",
    "hgb = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=None,\n",
    "        max_iter=400,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fb74e",
   "metadata": {},
   "source": [
    "# **8. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da78b74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTADOS (ordenados por logloss_test, menor es mejor):\n",
      "               model  logloss_train  logloss_test  auc_train  auc_test  acc_train  acc_test\n",
      "HistGradientBoosting       0.195743      0.205691   0.939461  0.925533   0.921452  0.919063\n",
      "  LogisticRegression       0.250158      0.241475   0.895807  0.903322   0.901660  0.904153\n",
      "    Baseline (prior)       0.437793      0.437474   0.500000  0.500000   0.841129  0.841321\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(name, pipe, X_tr, y_tr, X_te, y_te):\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    # Probabilidades\n",
    "    p_tr = pipe.predict_proba(X_tr)[:, 1]\n",
    "    p_te = pipe.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    # Predicción 0/1 por umbral 0.5 (solo para accuracy)\n",
    "    yhat_tr = (p_tr >= 0.5).astype(int)\n",
    "    yhat_te = (p_te >= 0.5).astype(int)\n",
    "\n",
    "    results = {\n",
    "        \"model\": name,\n",
    "        \"logloss_train\": log_loss(y_tr, p_tr, labels=[0, 1]),\n",
    "        \"logloss_test\":  log_loss(y_te, p_te, labels=[0, 1]),\n",
    "        \"auc_train\": roc_auc_score(y_tr, p_tr) if len(np.unique(y_tr)) == 2 else np.nan,\n",
    "        \"auc_test\":  roc_auc_score(y_te, p_te) if len(np.unique(y_te)) == 2 else np.nan,\n",
    "        \"acc_train\": accuracy_score(y_tr, yhat_tr),\n",
    "        \"acc_test\":  accuracy_score(y_te, yhat_te),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "results = []\n",
    "for name, model in [\n",
    "    (\"Baseline (prior)\", baseline),\n",
    "    (\"LogisticRegression\", logreg),\n",
    "    (\"HistGradientBoosting\", hgb),\n",
    "]:\n",
    "    results.append(evaluate_model(name, model, X_train, y_train, X_test, y_test))\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"logloss_test\")\n",
    "print(\"\\nRESULTADOS (ordenados por logloss_test, menor es mejor):\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e67f8b",
   "metadata": {},
   "source": [
    "# **9. DETECTAR OVERFITTING (train vs test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d6ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gap de log-loss (test - train):\n",
      "               model  logloss_train  logloss_test  logloss_gap\n",
      "HistGradientBoosting       0.195743      0.205691     0.009948\n",
      "    Baseline (prior)       0.437793      0.437474    -0.000319\n",
      "  LogisticRegression       0.250158      0.241475    -0.008683\n",
      "\n",
      "No se ve una señal fuerte de overfitting con esta heurística.\n"
     ]
    }
   ],
   "source": [
    "# Señal típica: logloss_train mucho menor que logloss_test (gap grande).\n",
    "results_df[\"logloss_gap\"] = results_df[\"logloss_test\"] - results_df[\"logloss_train\"]\n",
    "print(\"\\nGap de log-loss (test - train):\")\n",
    "print(results_df[[\"model\", \"logloss_train\", \"logloss_test\", \"logloss_gap\"]].sort_values(\"logloss_gap\", ascending=False).to_string(index=False))\n",
    "\n",
    "# Heurística simple (ajústala): gap > 0.05 puede ser sospechoso en muchos problemas\n",
    "SUSPECT_GAP = 0.05\n",
    "suspects = results_df[results_df[\"logloss_gap\"] > SUSPECT_GAP][\"model\"].tolist()\n",
    "if suspects:\n",
    "    print(\"\\nModelos sospechosos de overfitting (gap grande):\", suspects)\n",
    "else:\n",
    "    print(\"\\nNo se ve una señal fuerte de overfitting con esta heurística.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddd029",
   "metadata": {},
   "source": [
    "# **10. Stopping Criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e80da620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor modelo actual: HistGradientBoosting\n",
      "logloss_test = 0.2057\n",
      "\n",
      "Regla práctica:\n",
      "- Solo cambiar/complicar el modelo si mejora logloss_test al menos 0.002 y sin aumentar claramente el gap.\n"
     ]
    }
   ],
   "source": [
    "# Para este flujo: \"paro\" cuando el mejor modelo NO mejora logloss_test vs el anterior\n",
    "# por un margen mínimo (delta) y el gap no empeora.\n",
    "MIN_IMPROVEMENT = 0.002  # mejora mínima en log-loss para considerar que vale la pena\n",
    "best = results_df.iloc[0]\n",
    "print(\"\\nMejor modelo actual:\", best[\"model\"])\n",
    "print(f\"logloss_test = {best['logloss_test']:.4f}\")\n",
    "\n",
    "print(\"\\nRegla práctica:\")\n",
    "print(f\"- Solo cambiar/complicar el modelo si mejora logloss_test al menos {MIN_IMPROVEMENT} y sin aumentar claramente el gap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3efb702",
   "metadata": {},
   "source": [
    "# **11. Repeated Holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaaa2303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados por split:\n",
      "   seed  logloss_train  logloss_test       gap                 model\n",
      "0     0       0.247691      0.251067  0.003375    LogisticRegression\n",
      "1     0       0.193794      0.214918  0.021124  HistGradientBoosting\n",
      "2     1       0.247760      0.250638  0.002877    LogisticRegression\n",
      "3     1       0.194230      0.227998  0.033768  HistGradientBoosting\n",
      "4     2       0.247698      0.252312  0.004614    LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "SEEDS = range(10)        # número de repeticiones (puedes subir a 20 después)\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "def evaluate_once(seed, model, X, y):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=TEST_SIZE,\n",
    "        stratify=y,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    p_tr = model.predict_proba(X_tr)[:, 1]\n",
    "    p_te = model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"seed\": seed,\n",
    "        \"logloss_train\": log_loss(y_tr, p_tr, labels=[0, 1]),\n",
    "        \"logloss_test\": log_loss(y_te, p_te, labels=[0, 1]),\n",
    "        \"gap\": log_loss(y_te, p_te, labels=[0, 1]) - log_loss(y_tr, p_tr, labels=[0, 1])\n",
    "    }\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    res_lr = evaluate_once(seed, logreg, X, y)\n",
    "    res_lr[\"model\"] = \"LogisticRegression\"\n",
    "    results.append(res_lr)\n",
    "\n",
    "    res_hgb = evaluate_once(seed, hgb, X, y)\n",
    "    res_hgb[\"model\"] = \"HistGradientBoosting\"\n",
    "    results.append(res_hgb)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nResultados por split:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5869740",
   "metadata": {},
   "source": [
    "# **11.1 Statistical Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90dfac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de validación repetida:\n",
      "                      logloss_test_mean  logloss_test_std  gap_mean   gap_std  n_splits\n",
      "model                                                                                  \n",
      "HistGradientBoosting           0.218446          0.011100  0.025058  0.014006        10\n",
      "LogisticRegression             0.251424          0.007128  0.003199  0.008914        10\n"
     ]
    }
   ],
   "source": [
    "summary = (\n",
    "    results_df\n",
    "    .groupby(\"model\")\n",
    "    .agg(\n",
    "        logloss_test_mean=(\"logloss_test\", \"mean\"),\n",
    "        logloss_test_std=(\"logloss_test\", \"std\"),\n",
    "        gap_mean=(\"gap\", \"mean\"),\n",
    "        gap_std=(\"gap\", \"std\"),\n",
    "        n_splits=(\"logloss_test\", \"count\")\n",
    "    )\n",
    "    .sort_values(\"logloss_test_mean\")\n",
    ")\n",
    "\n",
    "print(\"\\nResumen de validación repetida:\")\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2eb9487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl HistGradientBoosting sigue siendo claramente superior en promedio,\\npero muestra algo más de variabilidad y un gap mayor que la regresión logística,\\naunque sin señales críticas de sobreajuste.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "El HistGradientBoosting sigue siendo claramente superior en promedio,\n",
    "pero muestra algo más de variabilidad y un gap mayor que la regresión logística,\n",
    "aunque sin señales críticas de sobreajuste.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38896d",
   "metadata": {},
   "source": [
    "# **12. Interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03d220",
   "metadata": {},
   "source": [
    "## **12.1 Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269dadcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            feature       coef\n",
      "1              Book_length_mins_avg   2.490939\n",
      "3                         Price_avg   1.454756\n",
      "8                  Support_requests   1.072270\n",
      "4                            Review   0.683568\n",
      "9  Last_visited_Minus_Purchase_date   0.530067\n",
      "5                      Review 10/10   0.062052\n",
      "7                        Completion  -0.057207\n",
      "2                     Price_overall  -1.208013\n",
      "0          Book_length_mins_overall  -2.047615\n",
      "6                  Minutes_listened -11.778497\n"
     ]
    }
   ],
   "source": [
    "# Extraer coeficientes de la Logistic Regression\n",
    "feature_names = logreg.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "coefs = logreg.named_steps[\"model\"].coef_[0]\n",
    "\n",
    "coef_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coef\": coefs\n",
    "    })\n",
    "    .sort_values(\"coef\", ascending=False)\n",
    ")\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d8f5a",
   "metadata": {},
   "source": [
    "## **12.2 HistGradientBoosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            feature  importance_mean  importance_std\n",
      "4                            Review        -0.006868        0.001006\n",
      "7                        Completion        -0.007951        0.001200\n",
      "0          Book_length_mins_overall        -0.011441        0.001561\n",
      "2                     Price_overall        -0.020985        0.001576\n",
      "8                  Support_requests        -0.036538        0.005389\n",
      "9  Last_visited_Minus_Purchase_date        -0.064472        0.003806\n",
      "1              Book_length_mins_avg        -0.082967        0.007632\n",
      "3                         Price_avg        -0.088929        0.004865\n",
      "5                      Review 10/10        -0.242586        0.013259\n",
      "6                  Minutes_listened        -0.687587        0.031926\n"
     ]
    }
   ],
   "source": [
    "# Importancia de features del HGB\n",
    "perm = permutation_importance(\n",
    "    hgb,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=20,\n",
    "    random_state=42,\n",
    "    scoring=\"neg_log_loss\"\n",
    ")\n",
    "\n",
    "feature_names = hgb.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "\n",
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": -perm.importances_mean,  # negativo → positivo = empeora\n",
    "        \"importance_std\": perm.importances_std\n",
    "    })\n",
    "    .sort_values(\"importance_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(importance_df)\n",
    "# Más negativo = más importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df092e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Interpretación:\n",
    "La probabilidad de recompra está dominada por engagement real (minutos escuchados) y señales extremas de satisfacción (review 10/10). El precio promedio y la longitud típica del contenido modulan la decisión, mientras que métricas más genéricas (review promedio, completion) aportan poco una vez controlado el engagement.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8fad5",
   "metadata": {},
   "source": [
    "# **13. Using the Model for Decision Making**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c81762c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book_length_mins_overall</th>\n",
       "      <th>Book_length_mins_avg</th>\n",
       "      <th>Price_overall</th>\n",
       "      <th>Price_avg</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review 10/10</th>\n",
       "      <th>Minutes_listened</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Support_requests</th>\n",
       "      <th>Last_visited_Minus_Purchase_date</th>\n",
       "      <th>p_repurchase</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>1656.0</td>\n",
       "      <td>4968</td>\n",
       "      <td>6.22</td>\n",
       "      <td>18.66</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>475.2</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1674.0</td>\n",
       "      <td>3348</td>\n",
       "      <td>6.09</td>\n",
       "      <td>12.17</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>1134.0</td>\n",
       "      <td>2268</td>\n",
       "      <td>6.93</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1656.0</td>\n",
       "      <td>4968</td>\n",
       "      <td>7.11</td>\n",
       "      <td>21.32</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>1890.0</td>\n",
       "      <td>3780</td>\n",
       "      <td>8.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Book_length_mins_overall  Book_length_mins_avg  Price_overall  Price_avg  Review  Review 10/10  \\\n",
       "324                     1656.0                  4968           6.22      18.66       0          8.91   \n",
       "1315                    1674.0                  3348           6.09      12.17       0          8.91   \n",
       "1383                    1134.0                  2268           6.93      13.87       0          8.91   \n",
       "319                     1656.0                  4968           7.11      21.32       1         10.00   \n",
       "1879                    1890.0                  3780           8.00      16.00       0          8.91   \n",
       "\n",
       "      Minutes_listened  Completion  Support_requests  Last_visited_Minus_Purchase_date  p_repurchase  actual  \n",
       "324                0.0       475.2                 1                               141      0.995655       1  \n",
       "1315               0.0         0.0                 0                               247      0.995655       1  \n",
       "1383               0.0         0.0                 0                                30      0.995655       1  \n",
       "319                0.0       486.0                 0                                18      0.995655       1  \n",
       "1879               0.0         0.0                 0                                 0      0.995655       1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir el score operativo\n",
    "\n",
    "p_test = hgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_scores = X_test.copy()\n",
    "df_scores[\"p_repurchase\"] = p_test\n",
    "df_scores[\"actual\"] = y_test.values\n",
    "\n",
    "df_scores.sort_values(\"p_repurchase\", ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa4434d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "segment\n",
       "Low       0.839901\n",
       "High      0.082712\n",
       "Medium    0.077387\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmentación por probabilidad\n",
    "\n",
    "df_scores[\"segment\"] = pd.cut(\n",
    "    df_scores[\"p_repurchase\"],\n",
    "    bins=[0, 0.3, 0.6, 1.0],\n",
    "    labels=[\"Low\", \"Medium\", \"High\"]\n",
    ")\n",
    "\n",
    "df_scores[\"segment\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "786661bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16359/1406322114.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_scores.groupby(\"segment\")[\"actual\"].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "segment\n",
       "Low       0.049451\n",
       "Medium    0.463303\n",
       "High      0.982833\n",
       "Name: actual, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluar lift por segmento:\n",
    "df_scores.groupby(\"segment\")[\"actual\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Construimos un modelo de probabilidad de recompra validado fuera de muestra, identificamos las variables de engagement que lo impulsan, y lo convertimos en un sistema de segmentación accionable para optimizar campañas y evitar gasto ineficiente.\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_365_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
